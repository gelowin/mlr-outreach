---
title: "mlr3 Workshop"
author: "Jakob Richter"
date: "September 27, 2019"
output: html_document
---

# mlr3 Workshop

## mlr3 building blocks

### Read The dataset and remove unsuitable columns

```{r}
library(data.table)
# Read data and convert strings to factors (as most learners cant handle string columns)
titanic = fread("https://gist.githubusercontent.com/jakob-r/e97e4174534c1d6a3fc95758c6cdc290/raw/fb4795eeb40c03663837a89cf2f97cece52345a2/train.csv", na.strings = "", stringsAsFactors = TRUE)
titanic_pred = fread("https://gist.githubusercontent.com/jakob-r/e97e4174534c1d6a3fc95758c6cdc290/raw/fb4795eeb40c03663837a89cf2f97cece52345a2/test.csv", na.strings = "", stringsAsFactors = TRUE)
str(titanic)

# remove columns that are not directly suitable for prediction
remove_columns = c("PassengerId", "Name", "Ticket", "Cabin")
titanic = titanic[, !(colnames(titanic) %in% remove_columns), with = FALSE]
titanic_pred = titanic_pred[, !(colnames(titanic_pred) %in% remove_columns), with = FALSE]

# convert target column to factor (classification)
titanic[, Survived := as.factor(Survived)]

# build second data.table without columns that contain NAs
na_cols = sapply(colnames(titanic_pred), function(x) any(is.na(titanic[[x]])) || any(is.na(titanic_pred[[x]])))
na_cols = names(na_cols)[na_cols] # extract the names of cols with NAs
titanic_nona = titanic[, !(colnames(titanic) %in% na_cols), with = FALSE]
titanic_nona_pred = titanic_pred[, !(colnames(titanic_pred) %in% na_cols), with = FALSE]
```

### Define an mlr3 Task

```{r}
# if mlr3 is not installed, run the following line:
# remotes::install_github("mlr-org/mlr3verse")
library(mlr3verse) # loads all packages mlr3, mlr3tuning, mlr3learners, mlr3pipelines, mlr3viz etc.

# Build Titanic Task
titanic_tsk = TaskClassif$new(id = "titanic", backend = titanic, target = "Survived")
titanic_tsk

# Build Titanic Task without columsn that have NAs
titanic_nona_tsk = TaskClassif$new(id = "titanic_nona", backend = titanic_nona, target = "Survived")
titanic_nona_tsk
```

### Define an mlr3 Learner

Define a logisitc regression learner
```{r}
# Check available learners
mlr_learners
# get more informations from the tabular data
as.data.table(mlr_learners)

lrn = mlr_learners$get("classif.log_reg") # get item from learners directory
lrn = lrn("classif.log_reg") # shorter way
```

### Train the Learner on the Task

Train the previously defined learner on the `titanic_tsk` task.
```{r}
lrn$train(task = titanic_nona_tsk)
```

The learner now contains the trained model:
```{r}
class(lrn$model)
summary(lrn$model)
```

### Make Predictions

Use the learner that now stores the model to predict the label on the `titanic_nona_pred` data.
```{r}
# ?Learner
pred = lrn$predict_newdata(task = titanic_nona_tsk, newdata = titanic_nona_pred)
(pred_dt = as.data.table(pred))
table(pred_dt$response)
```
As `titanic_nona_pred` has no labeled observations we can not calculate the performance.

This can only be done on the training data.

Define a split for the training data:
```{r}
n = titanic_nona_tsk$nrow
train_inds = sample(seq_len(n), size = round(n * 0.66))
test_inds = setdiff(seq_len(n), train_inds)
```

Now we can predict on data with known labels and calculate various performance measures:
```{r}
lrn$predict_type
lrn$predict_type = "prob" # change predict type to probabilities

lrn$train(task = titanic_nona_tsk, row_ids = train_inds)
pred = lrn$predict(task = titanic_nona_tsk, row_ids = test_inds)
mlr_measures
msrs = lapply(c("classif.acc", "classif.auc", "time_train"), msr)
pred$score(measures = msrs)
```

## Resampling

To automate the test train split we can use resample.
First use a resampling method that generates one split like in our manual example before.
```{r}
rds = rsmp("cv")
rds$param_set$values
```
Now use a 5-fold cross-validation:
```{r}
rds$param_set$values$folds = 5
res = resample(task = titanic_nona_tsk, learner = lrn, resampling = rds)
res$score(msrs)
res$aggregate(msrs)
```

## Benchmarking

Benchmark 3 Learners on the `titanic.tsk.nona` and the `iris.task`.
The first learner should be `classif.kknn`.
The second learner should be `classif.kknn` as well but with another hyperparameter setting (e.g. `k = 1`).
Hint: You need to change the id for the benchmark function!
The third one can be chosen freely.
Hint: Use `listLearners(titanic.tsk)` to show possible choices for the task.
```{r}
set.seed(123)

# show learners with properties (again)
as.data.table(mlr_learners)
# experiments with learners that can not handle NAs
design_nona = benchmark_grid(
  tasks = titanic_nona_tsk, 
  learners = lapply(c("classif.log_reg", "classif.kknn", "classif.ranger"), lrn), 
  resamplings = rds)
design_with_na = benchmark_grid(
  tasks = titanic_tsk, 
  learners = lapply(c("classif.xgboost", "classif.rpart"), lrn), 
  resamplings = rds)
design_complete = rbind(design_nona, design_with_na)
res = benchmark(design = design_complete)
mlr3viz::autoplot(res)
```

## Tuning

We want to tune rpart decision tree

```{r}
library(mlr3tuning)
lrn_rpart = lrn("classif.rpart")
lrn_rpart$param_set
par_set = ParamSet$new(params = list(
  lrn_rpart$param_set$params$cp,
  ParamInt$new(id = "minsplit", lower = 1, upper = 40)
))

mlr_terminators
term_secs = term("clock_time")
term_secs$param_set
term_secs$param_set$values$secs = 3 # we allow the tuning for 3 seconds in each outer cross validation

mlr_tuners
tune_rs = tnr("random_search")

rpart_tuned = AutoTuner$new(learner = lrn_rpart, resampling = rsmp("cv"), measures = msr("classif.acc"), tune_ps = par_set, terminator = term_secs, tuner = tune_rs)

rsmpl_res = resample(titanic_tsk, rpart_tuned, rds, store_models = TRUE)
rsmpl_res$learners[[1]]$model$tuning_instance$result$tune_x
rsmpl_res$learners[[2]]$model$tuning_instance$result$tune_x

res2 = benchmark(design = benchmark_grid(titanic_tsk, rpart_tuned, rds))
res$combine(res2)
mlr3viz::autoplot(res)
```

## mlr3 pipelines

### Simple GraphLearners

Add imputation strategies to the learners that can not handle missing values.
```{r}
lrns_nomissing = lapply(c("classif.log_reg", "classif.kknn", "classif.ranger"), lrn)
as.data.table(mlr_pipeops) # show available pipeops
lrns_sampleimp = lapply(lrns_nomissing, function(lrn) {
  pipe = po("imputesample") %>>% po("learner", learner = lrn) 
  GraphLearner$new(pipe)
})

res3 = resample(titanic_tsk, lrns_sampleimp, resampling = rds)
res$combine(as_benchmark_result(design_impute))
library(ggplot2)
mlr3viz::autoplot(res) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Find the best imputation strategy including hyperpars:
```{r}
impute_pos = as.data.table(mlr_pipeops)[grepl("impute", key)] # show available pipeops
impute_pos
impute_nums = c("imputehist", "imputemedian", "imputemean") #imputing strategies for numerical values
impute_fcts = c("imputenewlvl", "imputesample") #... for categorical (factor) values

# Branching pipeops for numerical and factor imputation
po_branch_nums = po("branch", options = impute_nums, id = "brnch_nums")
po_branch_fcts = po("branch", options = impute_fcts, id = "brnch_fcts")

# Pipeops for numerical imputation
pos_impute_nums = lapply(impute_nums, po)
pos_impute_nums = gunion(pos_impute_nums)

# Pipeops for factor imputation
pos_impute_fcts = lapply(impute_fcts, po)
pos_impute_fcts = gunion(pos_impute_fcts)

# Build complete pipe
pipe = po_branch_nums %>>% pos_impute_nums %>>% po("unbranch", id = "unbr_nums") %>>% 
  po_branch_fcts %>>% pos_impute_fcts %>>% po("unbranch", id = "unbr_fcts") %>>% 
  po("learner", learner = lrn("classif.ranger", num.trees = 200))
plot(pipe)
grph_lrn = GraphLearner$new(pipe)

# Define Tuning

# ParamSet
grph_lrn$param_set
par_set = ParamSet$new(params = list(
  grph_lrn$param_set$params$brnch_nums.selection,
  grph_lrn$param_set$params$brnch_fcts.selection,
  ParamInt$new(id = "classif.ranger.mtry", lower = 1, upper = titanic_tsk$ncol-1),
  ParamInt$new(id = "classif.ranger.min.node.size", lower = 1, upper = 20)
))

# Tuning Budget
term_evals = term("evals", n_evals = 20)

# Tuning Instance
instance = TuningInstance$new(
  task = titanic_tsk,
  learner = grph_lrn,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  param_set = par_set,
  terminator = term_evals
)

# Construct and start tuner
tuner = tnr("random_search")
tuner$tune(instance)
instance$result
instance$archive(unnest = "tune_x")
```

With a nested cross-validation setting we can assess the performance of the tuned random forest.

```{r}
grph_lrn_at = AutoTuner$new(
  learner = grph_lrn, 
  resampling = rsmp("cv", folds = 3), 
  measures = msr("classif.ce"), 
  tune_ps = par_set, 
  terminator = term_evals,
  tuner = tnr("random_search")
)
grph_lrn_at$id = "rf_tuned_impute"
rsmpl_res_graph_at = resample(titanic_tsk, grph_lrn_at, rds) #rds is taken from above (rds = rsmp("cv"))
rsmpl_res_graph_at$aggregate()
res4 = as_benchmark_result(rsmpl_res_graph_at)
res$combine(res4)
mlr3viz::autoplot(res) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

### Building an advanced Learner



