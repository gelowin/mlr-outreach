<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>mlr3</title>
    <meta charset="utf-8" />
    <meta name="author" content="Michel Lang, Bernd Bischl, Jakob Richter, Patrick Schratz, Martin Binder" />
    <link href="libs/font-awesome/css/fontawesome-all.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="assets/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="assets/css/extra.css" type="text/css" />
    <link rel="stylesheet" href="assets/css/tachyons.min.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: middle left hide-count

&lt;img src="https://raw.githubusercontent.com/mlr-org/mlr/master/man/figures/logo_navbar.png"&gt;


.talk-meta[
.talk-title[
# Reproducible machine learning workflows
]

.talk-author[
Michel Lang, Bernd Bischl, **Jakob Richter**, **Patrick Schratz**, Martin Binder
]

.talk-location.i[
whyR conference, Warsaw
]

.talk-date.i[
September 27th, 2019
]
]

---

# What is machine learning?

Supervised 
* **Classification**
  * Binary
  * Multiclass
* **Regression**
* Survival (Time To Event)

Unsupervised
* Clustering

---

# What is a machine learning pipeline?

&lt;img src="assets/ml-workflow.png" width="90%" style="display: block; margin: auto;" /&gt;

<i class="fas  fa-question-circle "></i> Why are pipelines necessary?  

<i class="fas  fa-exclamation-circle "></i> Gives reproducible process that can be validated (through nested cross-validation).

---

# Machine Learning in R

The **good** news:
- CRAN serves hundreds of packages for machine learning
- Many packages for single specific ML-methods
  - `randomForest`, `ranger`
  - `e1070::svm`
  - `kknn`
  - `xgboost`
  - ...
- Often compliant to the unwritten interface definition:

  ```r
  model = fit(target ~ ., data = train.data, ...)
  predictions = predict(model, newdata = test.data, ...)
  ```

---

# Machine Learning in R

The **bad** news:
- Some packages' API is "just different"

- Functionality is always package or model-dependent, even though the procedure might be general

- Capabilities are often not clearly documented
  - "Can this method handle missing/categorical values?"
  
- ML workflow not standardized in R
  - Cross-Validation
  - Hyperparameter Tuning
  - Self implementations are error prone

---

# Motivation: (old) mlr

- Unified interface for the basic building blocks: **tasks**, **learners**,
  **hyperparameters**.
  
- Simplifying repetitive tasks:
  - cross-validation
  - hyperparameter tuning
  - comparison of different methods
  - parallelization  
  
- Project homepage: https://github.com/mlr-org/mlr
- 8-10 main developers
  - even more contributors
  - 5 GSOC projects since 2015
- About 30K lines of code, 8K lines of unit tests

---

# What (old) mlr became (1/2)

Meta framework for everything machine learning (visualization, tuning, feature selection, preprocessing, bagging, ensembles, ...)

* Monolithic package

* Interfaces &gt; 150 learners  

  <i class="fas  fa-arrow-right "></i> Dependencies (direct / recursive): 119 / 1436  
  <i class="fas  fa-arrow-right "></i> Unit tests take &gt; 2h  
  <i class="fas  fa-arrow-right "></i> Package developers changed their API and (unknowingly) broke mlr  
  
* High barrier for new contributors

---

# What (old) mlr became (2/2)

Meta framework for everything machine learning (visualization, tuning, feature selection, preprocessing, bagging, ensembles, ...)

* S3 reaches its limitations in larger software projects

* Many specialized and "hard to find" functions `getBMRAggrPerformances()`

* Only works on in-memory data

* No nested parallelization

---

# mlr3

Overcome limitations of S3 with the help of **R6**
  * Truly object-oriented (OO): data and methods together
  * Inheritance
  * Reference semantics
  
Embrace **data.table**, both for arguments and for internal data structures
  * Fast operations for tabular data
  * Better support for list columns to arrange complex objects in a tabular structure
  * Reference semantics
  
Be **light on dependencies**. Direct and recursive dependencies:
  * `R6`, `data.table`, `Metrics`, `lgr`
  * Some self-maintained packages (`backports`, `checkmate`, ...)

---

# Consequences

* steeper learning curve to learn mlr3

* multiple packages for different functionality
  * mlr3learners
  * mlr3pipelines
  * mlr3tuning
  * mlr3filters
  * mlr3viz
  * ...
  
* easier extendable
  * inherit classes and overwrite functionality

---
class: inverse, center, middle

# mlr3

---

# Building Blocks

&lt;img src="assets/ml_abstraction_colors.svg.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# Tasks

<i class="fas  fa-arrow-right "></i> Create your own task


```r
TaskClassif$new("iris", iris, target = "Species")
```

```
# &lt;TaskClassif:iris&gt; (150 x 5)
# * Target: Species
# * Properties: multiclass
# * Features (4):
#   - dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width
```
    
<i class="fas  fa-arrow-right "></i> Retrieve a predefined task from the task dictionary


```r
mlr_tasks
```

```
# &lt;DictionaryTask&gt; with 9 stored values
# Keys: boston_housing, german_credit, iris, mtcars, pima, sonar, spam, wine, zoo
```

```r
task = mlr_tasks$get("iris")
```

---

# Learner

<i class="fas  fa-arrow-right "></i> Retrieve a predefined learner from the learner dictionary


```r
mlr_learners
```

```
# &lt;DictionaryLearner&gt; with 21 stored values
# Keys: classif.debug, classif.featureless, classif.glmnet, classif.kknn, classif.lda,
#   classif.log_reg, classif.naive_bayes, classif.qda, classif.ranger, classif.rpart,
#   classif.svm, classif.xgboost, regr.featureless, regr.glmnet, regr.kknn, regr.km,
#   regr.lm, regr.ranger, regr.rpart, regr.svm, regr.xgboost
```

<i class="fas  fa-info-circle "></i> More learners are available after loading `mlr3learners`.

---

# Learner


```r
learner = lrn("classif.rpart")
learner
```

```
# &lt;LearnerClassifRpart:classif.rpart&gt;
# * Model: -
# * Parameters: xval=0
# * Packages: rpart
# * Predict Type: response
# * Feature types: logical, integer, numeric, character, factor, ordered
# * Properties: importance, missings, multiclass, selected_features, twoclass, weights
```

---

# Learner

<i class="fas  fa-arrow-right "></i> Querying and setting hyperparameters


```r
# query
learner$param_set
```

```
# ParamSet: 
#              id    class lower upper levels default value
# 1:     minsplit ParamInt     1   Inf             20      
# 2:           cp ParamDbl     0     1           0.01      
# 3:   maxcompete ParamInt     0   Inf              4      
# 4: maxsurrogate ParamInt     0   Inf              5      
# 5:     maxdepth ParamInt     1    30             30      
# 6:         xval ParamInt     0   Inf             10     0
```

```r
# set
learner$param_set$values = list(xval = 0, cp = 0.1)
```
    
---

# Learner

<i class="fas  fa-arrow-right "></i> Training

```r
task = tsk("iris")
learner$train(task, row_ids = seq(1, to = 150, by = 2))
```

<i class="fas  fa-info-circle "></i> This changes the learner in-place; the model is now stored inside the learner.

---

# Learner

<i class="fas  fa-arrow-right "></i> Accessing the learner model


```r
learner$model
```

```
# n= 75 
# 
# node), split, n, loss, yval, (yprob)
#       * denotes terminal node
# 
# 1) root 75 50 setosa (0.3333333 0.3333333 0.3333333)  
#   2) Petal.Length&lt; 2.45 25  0 setosa (1.0000000 0.0000000 0.0000000) *
#   3) Petal.Length&gt;=2.45 50 25 versicolor (0.0000000 0.5000000 0.5000000)  
#     6) Petal.Width&lt; 1.65 25  1 versicolor (0.0000000 0.9600000 0.0400000) *
#     7) Petal.Width&gt;=1.65 25  1 virginica (0.0000000 0.0400000 0.9600000) *
```

---

# Learner

<i class="fas  fa-smile "></i> Unified interface, to acess *feature importance*, *selected features* etc. for all learners. 


```r
learner$importance()
```

```
#  Petal.Width Petal.Length Sepal.Length  Sepal.Width 
#      46.1600      41.9280      26.4640      21.9248
```

```r
learner$selected_features()
```

```
# [1] "Petal.Length" "Petal.Width"
```

---

# Prediction

<i class="fas  fa-arrow-right "></i> Generate predictions or confusion matrices


```r
p = learner$predict(task, row_ids = seq(2, to = 150, by = 2))
head(as.data.table(p), 3)
```

```
#    row_id  truth response
# 1:      2 setosa   setosa
# 2:      4 setosa   setosa
# 3:      6 setosa   setosa
```


```r
p$confusion
```

```
#             truth
# response     setosa versicolor virginica
#   setosa         25          0         0
#   versicolor      0         24         3
#   virginica       0          1        22
```

---

# Measure

<i class="fas  fa-arrow-right "></i> Retrieve a predefined measure from the measure dictionary


```r
(measure = mlr_measures$get("classif.acc"))
```

```
# &lt;MeasureClassifACC:classif.acc&gt;
# * Packages: Metrics
# * Range: [0, 1]
# * Minimize: FALSE
# * Properties: -
# * Predict type: response
```

```r
mlr_measures
```

```
# &lt;DictionaryMeasure&gt; with 30 stored values
# Keys: classif.acc, classif.auc, classif.ce, classif.costs, classif.f1, classif.fdr,
#   classif.fn, classif.fnr, classif.for, classif.fp, classif.fpr, classif.npv,
#   classif.ppv, classif.precision, classif.recall, classif.sensitivity,
#   classif.specificity, classif.tn, classif.tnr, classif.tp, classif.tpr, debug,
#   oob_error, regr.mae, regr.mse, regr.rmse, selected_features, time_both, time_predict,
#   time_train
```

---

# Measure

<i class="fas  fa-arrow-right "></i> Calculate performance

```r
p$score(measure)
```

```
# classif.acc 
#   0.9466667
```

---

# Resample

<i class="fas  fa-arrow-right "></i> Automate train/test splitting and predict with *resampling*.


```r
mlr_resamplings
```

```
# &lt;DictionaryResampling&gt; with 6 stored values
# Keys: bootstrap, custom, cv, holdout, repeated_cv, subsampling
```

```r
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling)
```

---

# Resample


```r
rr$score() 
```

.code55[

```
#             task task_id               learner    learner_id     resampling resampling_id iteration
# 1: &lt;TaskClassif&gt;    iris &lt;LearnerClassifRpart&gt; classif.rpart &lt;ResamplingCV&gt;            cv         1
# 2: &lt;TaskClassif&gt;    iris &lt;LearnerClassifRpart&gt; classif.rpart &lt;ResamplingCV&gt;            cv         2
# 3: &lt;TaskClassif&gt;    iris &lt;LearnerClassifRpart&gt; classif.rpart &lt;ResamplingCV&gt;            cv         3
#    prediction classif.ce
# 1:     &lt;list&gt;       0.06
# 2:     &lt;list&gt;       0.04
# 3:     &lt;list&gt;       0.06
```
]


```r
rr$aggregate() 
```

```
# classif.ce 
# 0.05333333
```

<i class="fas  fa-info-circle "></i> The default measure for classif tasks is the *classification error*.

---

# Recap: Building Blocks

&lt;img src="assets/ml_abstraction_colors.svg.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# Dictionaries

Dictionaries store often used objects.

|Dictionary|Helper|  |
|:---------|:-----|--|
|`mlr_tasks`|`tsk()`|Example tasks (iris, spam, ...)|
|`mlr_task_generators`|`tgen()`|Synthetic task generators|
|`mlr_learners`|`lrn()`|learners|
|`mlr_measures`|`msr()`|measures (acc, auc, mse, ...)|
|`mlr_resamplings`|`rsmp()`|resampling strategies (cv, holdout, bootstrap, ...)|

<i class="fas  fa-info-circle "></i> Dictionaries can get populated by add-on packages (e.g. `mlr3learners`).

---

# Recap: Example Train/Test



.code75[

```r
# Create learning task
task_iris = TaskClassif$new(id = "iris", backend = iris, 
  target = "Species")

# Load learner from dictionary
learner = lrn("classif.rpart")

# Set learner hyperparameter
learner$param_set$values$cp = 0.01

# Create train/test split
train_set = sample(task_iris$nrow, 0.8 * task_iris$nrow)
test_set = setdiff(seq_len(task_iris$nrow), train_set)

# Train the model
learner$train(task_iris, row_ids = train_set)

# Predict data
prediction = learner$predict(task_iris, 
  row_ids = test_set)
```
]

---

# Example Train/Test (cont.)


```r
prediction
```

```
# &lt;PredictionClassif&gt; for 30 observations:
#     row_id     truth  response
#          4    setosa    setosa
#          5    setosa    setosa
#          8    setosa    setosa
# ---                           
#        128 virginica virginica
#        137 virginica virginica
#        139 virginica virginica
```

```r
# calculate performance
prediction$confusion
```

```
#             truth
# response     setosa versicolor virginica
#   setosa         11          0         0
#   versicolor      0         12         1
#   virginica       0          0         6
```

```r
prediction$score(msr("classif.acc"))
```

```
# classif.acc 
#   0.9666667
```

---

# Recap: Example Resample

<i class="fas  fa-arrow-right "></i> Resampling Object

.font25[

```r
cv3 = rsmp("cv", folds = 3)
```

Splits into train/test are efficiently stored and can be accessed with `$train_set(i)` and `$test_set(i)`.

<i class="fas  fa-arrow-right "></i> Resample a **regression tree** on the "Boston housing" data using a **3-fold CV**


```r
# string -&gt; object conversion via dictionary
rr = resample(tsk("boston_housing"), lrn("regr.rpart"), cv3)
```

<i class="fas  fa-arrow-right "></i> Aggregated performance


```r
rr$aggregate(msr("regr.mse"))
```

```
# regr.mse 
# 3.452013
```
]

---
class: inverse, center, middle

# Benchmarking

---

# Benchmarking

<i class="fas  fa-info-circle "></i> `benchmark()` runs `resample()` on multiple learners and  tasks

<i class="fas  fa-arrow-right "></i> A sensible choice is usually the combination of all components in an exhaustive grid:


```r
design = benchmark_grid(
  tasks = list(tsk("iris"), tsk("spam")), resamplings = rsmp("cv"),
  learners = list(lrn("classif.featureless"), lrn("classif.rpart"))
)
design
```

```
#             task                     learner     resampling
# 1: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingCV&gt;
# 2: &lt;TaskClassif&gt;       &lt;LearnerClassifRpart&gt; &lt;ResamplingCV&gt;
# 3: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingCV&gt;
# 4: &lt;TaskClassif&gt;       &lt;LearnerClassifRpart&gt; &lt;ResamplingCV&gt;
```

---

# Benchmarking (cont.)

<i class="fas  fa-arrow-right "></i> Execute `benchmark()` on the defined design


```r
bmr = benchmark(design, store_models = TRUE)
bmr
```

---

# Benchmarking (cont.)

<i class="fas  fa-arrow-right "></i> Calculate aggregated performance measure


```r
aggr = bmr$aggregate(msr("classif.acc"))
aggr[, 3:7]
```

---

# Benchmarking (cont.)

<i class="fas  fa-arrow-right "></i> Retrieve objects from the `BenchmarkResult`:


```r
# ResampleResult from 2nd configuration in the design 
# (rpart on iris)

rr = bmr$resample_result(2)

# confusion matrix
rr$prediction()$confusion
```

---

# Benchmarking (cont.)


```r
# Average feature importance
library(magrittr)
sapply(rr$data$learner, function(x) x$importance()) %&gt;% 
  apply(1, mean)
```

---
class: inverse, center, middle

# Tuning

---

# Tuning

* Algorithms: 
  - _Grid Search_
  - _Random Search_ 
  - _Simulated Annealing_

* In process
  - _Bayesian Optimization_
  - _iterated F-racing_
  - _EAs_

* Budget via class `Terminator`: iterations, performance, runtime, real time

* Nested resampling via class `AutoTuner`

---

# Tuning


```r
library(mlr3learners)
library(mlr3tuning)

ps = ParamSet$new(list(
  ParamInt$new("min.node.size", lower = 1, upper = 10),
  ParamInt$new("mtry", lower = 10, upper = 50)
))

at = AutoTuner$new(
  learner = lrn("classif.ranger"),
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.acc"),
  tune_ps = ps, 
  terminator = term("evals", n_evals = 2), 
  tuner = tnr("random_search")
)
```

---

# Tuning


```r
resample(tsk("spam"), learner = at, rsmp("holdout"))
```

```
# &lt;ResampleResult&gt; of 1 iterations
# * Task: spam
# * Learner: classif.ranger.tuned
# * Performance: 0.053 [classif.ce]
# * Warnings: 0 in 0 iterations
# * Errors: 0 in 0 iterations
```

---

# `AutoTuner` in Benchmark

.pull-left[
<i class="fas  fa-arrow-right "></i> Compare tuned learner vs. default learner:
.code75[

```r
design = benchmark_grid(
  tasks = tsk("spam"), 
  learners = list(
    at, 
    lrn("classif.ranger")
  ), 
  resamplings = rsmp("cv", 
    folds = 5))
bmr = benchmark(design)
```

<i class="fas  fa-info-circle "></i> _mlr3viz_ package contains `autoplot()` functions for some `mlr3` objects.


```r
library(mlr3viz)
autoplot(bmr)
```

]
]

.pull-right[

&lt;img src="whyr2019_mlr3_files/figure-html/fig_at_benchmark-1.svg" width="90%" style="display: block; margin: auto;" /&gt;
]

---

class: inverse, center, middle

# mlr3 Advanced Usage

---

# Internal Data Structure

All result objects (`resample()`, `benchmark()`, tuning, ...) share the same structure:


```r
as.data.table(rr)
```

```
#          task            learner     resampling iteration prediction
# 1: &lt;TaskRegr&gt; &lt;LearnerRegrRpart&gt; &lt;ResamplingCV&gt;         1     &lt;list&gt;
# 2: &lt;TaskRegr&gt; &lt;LearnerRegrRpart&gt; &lt;ResamplingCV&gt;         2     &lt;list&gt;
# 3: &lt;TaskRegr&gt; &lt;LearnerRegrRpart&gt; &lt;ResamplingCV&gt;         3     &lt;list&gt;
```

---

# Internal Data Structure

#### Combining R6 and data.table
* Not the objects are stored, but pointers to them

* Inexpensive to work on:
  * `rbind()`: copying R6 objects &amp;wedgeq; copying pointers
  * `cbind()`: `data.table()` over-allocates columns, no copies
  * `[i, ]`: lookup row (possibly hashed), create a list of pointers
  * `[, j]`: direct access to list element

---

# Control of Execution

<i class="fas  fa-arrow-right "></i> Parallelization

```r
future::plan("multicore")
benchmark(grid)
```
* runs each resampling iteration as a job&lt;br/&gt;
* also allows nested resampling (although not needed here)

<i class="fas  fa-arrow-right "></i>  Encapsulation


```r
ctrl = mlr_control(encapsulate_train = "callr")
benchmark(grid, ctrl = ctrl)
```
* Spawns a separate R process to train the learner
* Learner may segfault without tearing down the master session
* Logs are captured
* Possibility to have a fallback learner to create predictions

---

# Out-of-memory Data

* Task stores data in a `DataBackend`:
    * `DataBackendDataTable`: Default backend for dense data (in-memory)
    * `DataBackendMatrix`: Backend for sparse numerical data (in-memory)
    * `mlr3db::DataBackendDplyr`: Backend for many DBMS (out-of-memory)
    * `DataBackendCbind`: Combine backends thorugh `task$cbind(backend)` (virtual)
    * `DataBackendRbind`: Combine backends thorugh `task$rbind(backend)` (virtual)
    
* Backends are immutable
    * Filtering rows or selecting columns just modifies the "view" on the data
    * Multiple tasks can share the same backend
    
* Example: Interface a read-only MariaDB with `DataBackendDplyr`, add generated features via `DataBackendDataTable`

---

# Current state

https://github.com/mlr-org/mlr3/wiki/CI-Status
https://github.com/mlr-org/mlr3/wiki/Extension-Packages


Want to contribute?  
[mlr3.mlr-org.com](https://mlr3.mlr-org.com)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
