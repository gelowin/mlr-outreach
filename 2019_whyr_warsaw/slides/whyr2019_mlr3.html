<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>mlr3</title>
    <meta charset="utf-8" />
    <meta name="author" content="Michel Lang, Bernd Bischl, Jakob Richter, Patrick Schratz, Martin Binder" />
    <link href="whyr2019_mlr3_files/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="whyr2019_mlr3_files/remark-css-0.0.1/robot-fonts.css" rel="stylesheet" />
    <link href="whyr2019_mlr3_files/font-awesome-5.3.1/css/fontawesome-all.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="extra.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# mlr3
## Modern machine learning in R
### Michel Lang, Bernd Bischl, Jakob Richter, Patrick Schratz, Martin Binder

---



# What is machine learning?

Supervised 
* **Classification**
  * Binary
  * Multiclass
* **Regression**
* Survival (Time To Event)

Unsupervised
* Clustering

---

# What is a machine learning pipeline?

&lt;img src="assets/ml-workflow.png" width="90%" style="display: block; margin: auto;" /&gt;

<i class="fas  fa-question-circle "></i> Why are pipelines necessary?  
<i class="fas  fa-exclamation-circle "></i> Gives reproducible process that can be validated (through nested crossvalidation).


---

# Machine Learning in R

The **good** news:
- CRAN serves hundreds of packages for machine learning
- Many packages for single specific ML-methods
  - _randomForest_, _ranger_
  - _e1070::svm_
  - _kknn_
  - _xgboost_
  - ...
- Often compliant to the unwritten interface definition:


```r
model = fit(target ~ ., data = train.data, ...)
predictions = predict(model, newdata = test.data, ...)
```

---

# Machine Learning in R

The **bad** news:
- Some packages' API is "just different"
- Functionality is always package or model-dependent, even though the procedure might be general
- Capabilities are often not clearly documented
  - "Can this method handle missing/categorical values?"
- ML workflow not standardized in R
  - Cross-Validation
  - Hyperparameter Tuning
  - Self implementations are error prone

---

# Motivation: (old) mlr

- Unified interface for the basic building blocks: **tasks**, **learners**,
  **hyperparameters**.

- Simplifying repetitive tasks:
  - cross-validation
  - hyperparameter tuning
  - comparison of different methods
  - parallelization  

- Project home page: https://github.com/mlr-org/mlr
- 8-10 main developers
  - + even more contributors
  - + 5 GSOC projects since 2015
- About 30K lines of code, 8K lines of unit tests

---

# What (old) mlr became

Meta framework for everything machine learning (visualization, tuning, feature selection, preprocessing, bagging, ensembles, ...)

* Monolithic package
* Interfaces &gt; 150 learners  
  <i class="fas  fa-arrow-right "></i> Dependencies (direct / recursive): 119 / 1436  
  <i class="fas  fa-arrow-right "></i> Unit tests take &gt; 2h  
  <i class="fas  fa-arrow-right "></i> Package developers changed their API and (unknowingly) broke mlr  
* High barrier for new contributors
* S3 reaches its limitations in larger software projects
* Many specialized and "hard to find" functions `getBMRAggrPerformances()`

#### Further Design Issues

* Only works on in-memory data
* No nested parallelization

---

# mlr3

* Overcome limitations of S3 with the help of **R6**
  * Truly object-oriented (OO): data and methods together
  * Inheritance
  * Reference semantics
  
* Embrace **data.table**, both for arguments and for internal data structures
  * Fast operations for tabular data
  * Better support for list columns to arrange complex objects in a tabular structure
  * Reference semantics
  
* Be **light on dependencies**. Direct and recursive dependencies:
  * `R6`, `data.table`, `Metrics`, `lgr`
  * Some self-maintained packages (`backports`, `checkmate`, ...)

---

# Consequences

* steeper learning curve

* multiple packages for different functionality
  * _mlr3learners_
  * _mlr3pipelines_
  * _mlr3tuning_
  * _mlr3filters_
  * _mlr3viz_
  * ...
  
* easier extendable
  * inherit classes and overwrite functionality

---
class: inverse, center, middle

# mlr3

---

# Building Blocks
&lt;img src="assets/ml_abstraction_colors.svg.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# Tasks

<i class="fas  fa-arrow-right "></i> Create your own task

```r
TaskClassif$new("iris", iris, target = "Species")
```

```
&lt;TaskClassif:iris&gt; (150 x 5)
* Target: Species
* Properties: multiclass
* Features (4):
  - dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width
```
    
<i class="fas  fa-arrow-right "></i> Retrieve a predefined task from the task dictionary


```r
mlr_tasks
```

```
&lt;DictionaryTask&gt; with 9 stored values
Keys: boston_housing, german_credit, iris, mtcars, pima, sonar, spam, wine, zoo
```

```r
task = mlr_tasks$get("iris")
```

---

# Learner

<i class="fas  fa-arrow-right "></i> Retrieve a predefined learner from the learner dictionary


```r
mlr_learners
```

```
&lt;DictionaryLearner&gt; with 21 stored values
Keys: classif.debug, classif.featureless, classif.glmnet, classif.kknn, classif.lda,
  classif.log_reg, classif.naive_bayes, classif.qda, classif.ranger, classif.rpart,
  classif.svm, classif.xgboost, regr.featureless, regr.glmnet, regr.kknn, regr.km,
  regr.lm, regr.ranger, regr.rpart, regr.svm, regr.xgboost
```
<i class="fas  fa-info-circle "></i> More learners are available after loading `mlr3learners`.


```r
learner = mlr_learners$get("classif.rpart")
learner
```

```
&lt;LearnerClassifRpart:classif.rpart&gt;
* Model: -
* Parameters: xval=0
* Packages: rpart
* Predict Type: response
* Feature types: logical, integer, numeric, character, factor, ordered
* Properties: importance, missings, multiclass, selected_features, twoclass, weights
```

---
# Learner

<i class="fas  fa-arrow-right "></i> Querying and setting hyperparameters


```r
# query
learner$param_set
```

```
ParamSet: 
             id    class lower upper levels default value
1:     minsplit ParamInt     1   Inf             20      
2:           cp ParamDbl     0     1           0.01      
3:   maxcompete ParamInt     0   Inf              4      
4: maxsurrogate ParamInt     0   Inf              5      
5:     maxdepth ParamInt     1    30             30      
6:         xval ParamInt     0   Inf             10     0
```

```r
# set
learner$param_set$values = list(xval = 0, cp = 0.1)
```
    
---

# Learner

<i class="fas  fa-arrow-right "></i> Training

```r
task = mlr_tasks$get("iris")
learner$train(task, row_ids = seq(1, to = 150, by = 2))
```

<i class="fas  fa-info-circle "></i> This changes the learner in-place; the model is now stored inside the learner.

---

# Learner

<i class="fas  fa-arrow-right "></i> Accessing the learner model


```r
learner$model
```

```
n= 75 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 75 50 setosa (0.3333333 0.3333333 0.3333333)  
  2) Petal.Length&lt; 2.45 25  0 setosa (1.0000000 0.0000000 0.0000000) *
  3) Petal.Length&gt;=2.45 50 25 versicolor (0.0000000 0.5000000 0.5000000)  
    6) Petal.Width&lt; 1.65 25  1 versicolor (0.0000000 0.9600000 0.0400000) *
    7) Petal.Width&gt;=1.65 25  1 virginica (0.0000000 0.0400000 0.9600000) *
```

<i class="fas  fa-smile "></i> Unified interface, to access *feature importance*, *selected features* etc. for all learners. 


```r
learner$importance()
```

```
 Petal.Width Petal.Length Sepal.Length  Sepal.Width 
     46.1600      41.9280      26.4640      21.9248 
```

```r
learner$selected_features()
```

```
[1] "Petal.Length" "Petal.Width" 
```

---
# Prediction

<i class="fas  fa-arrow-right "></i> Generate predictions


```r
p = learner$predict(task, row_ids = seq(2, to = 150, by = 2))
head(as.data.table(p), 3)
```

```
   row_id  truth response
1:      2 setosa   setosa
2:      4 setosa   setosa
3:      6 setosa   setosa
```

<i class="fas  fa-arrow-right "></i> Confusion matrix

```r
p$confusion
```

```
            truth
response     setosa versicolor virginica
  setosa         25          0         0
  versicolor      0         24         3
  virginica       0          1        22
```

---

# Measure
<i class="fas  fa-arrow-right "></i> Retrieve a predefined measure from the measure dictionary



```r
mlr_measures
```

```
&lt;DictionaryMeasure&gt; with 30 stored values
Keys: classif.acc, classif.auc, classif.ce, classif.costs, classif.f1, classif.fdr, classif.fn,
  classif.fnr, classif.for, classif.fp, classif.fpr, classif.npv, classif.ppv, classif.precision,
  classif.recall, classif.sensitivity, classif.specificity, classif.tn, classif.tnr, classif.tp,
  classif.tpr, debug, oob_error, regr.mae, regr.mse, regr.rmse, selected_features, time_both, time_predict,
  time_train
```

```r
(measure = mlr_measures$get("classif.acc"))
```

```
&lt;MeasureClassifACC:classif.acc&gt;
* Packages: Metrics
* Range: [0, 1]
* Minimize: FALSE
* Properties: -
* Predict type: response
```


<i class="fas  fa-arrow-right "></i> Calculate performance

```r
p$score(measure)
```

```
classif.acc 
  0.9466667 
```

---

# Resample

<i class="fas  fa-arrow-right "></i> Automate train/test splitting and predict with *resampling*.
.small[

```r
mlr_resamplings
```

```
&lt;DictionaryResampling&gt; with 6 stored values
Keys: bootstrap, custom, cv, holdout, repeated_cv, subsampling
```

```r
resampling = mlr_resamplings$get("cv")
resampling$param_set$values$folds = 3
rr = resample(task, learner, resampling)
rr$score()
```

```
            task task_id               learner    learner_id     resampling resampling_id iteration
1: &lt;TaskClassif&gt;    iris &lt;LearnerClassifRpart&gt; classif.rpart &lt;ResamplingCV&gt;            cv         1
2: &lt;TaskClassif&gt;    iris &lt;LearnerClassifRpart&gt; classif.rpart &lt;ResamplingCV&gt;            cv         2
3: &lt;TaskClassif&gt;    iris &lt;LearnerClassifRpart&gt; classif.rpart &lt;ResamplingCV&gt;            cv         3
   prediction classif.ce
1:     &lt;list&gt;       0.06
2:     &lt;list&gt;       0.06
3:     &lt;list&gt;       0.04
```

```r
rr$aggregate()
```

```
classif.ce 
0.05333333 
```
]

<i class="fas  fa-info-circle "></i> The default measure for classif tasks is *classification error*.

---
# Recap: Builing Blocks


---

# Dictionaries

Dictionaries store often used objects.

|Dictionary|Helper|  |
|:---------|:-----|--|
|`mlr_tasks`|`tsk()`|Example tasks (iris, spam, ...)|
|`mlr_task_generators`|`tgen()`|Synthetic task generators|
|`mlr_learners`|`lrn()`|learners|
|`mlr_measures`|`msr()`|measures (acc, auc, mse, ...)|
|`mlr_resamplings`|`rsmp()`|resampling strategies (cv, holdout, bootstrap, ...)|


<i class="fas  fa-info-circle "></i> Dictionaries can get populated by add-on packages (e.g. `mlr3learners`).


```r
mlr_tasks
```

```
&lt;DictionaryTask&gt; with 9 stored values
Keys: boston_housing, german_credit, iris, mtcars, pima, sonar, spam, wine, zoo
```

```r
task = mlr_tasks$get("spam")
task = tsk("spam") # helper
```

---

# Recap: Example Train/Test




```r
# Create learning task
task_iris = TaskClassif$new(id = "iris", backend = iris, 
  target = "Species")

# Load learner from dictionary
learner = lrn("classif.rpart")

# Set learner hyperparameter
learner$param_set$values$cp = 0.01

# Create train/test split
train_set = sample(task_iris$nrow, 0.8 * task_iris$nrow)
test_set = setdiff(seq_len(task_iris$nrow), train_set)

# Train the model
learner$train(task_iris, row_ids = train_set)

# Predict data
prediction = learner$predict(task_iris, row_ids = test_set)
```

---

# Example Train/Test (cont.)


```r
prediction
```

```
&lt;PredictionClassif&gt; for 30 observations:
    row_id     truth  response
         4    setosa    setosa
         5    setosa    setosa
         8    setosa    setosa
---                           
       128 virginica virginica
       137 virginica virginica
       139 virginica virginica
```

```r
# calculate performance
prediction$confusion
```

```
            truth
response     setosa versicolor virginica
  setosa         11          0         0
  versicolor      0         12         1
  virginica       0          0         6
```

```r
prediction$score(msr("classif.acc"))
```

```
classif.acc 
  0.9666667 
```

---
# Recap: Example Resample

<i class="fas  fa-arrow-right "></i> Resampling Object


```r
cv3 = rsmp("cv", folds = 3)
```

Splits into train/test are efficiently stored and can be accessed with `$train_set(i)` and `$test_set(i)`.

<i class="fas  fa-arrow-right "></i> Resample a **regression tree** on the "Boston housing" data using a **3-fold CV**


```r
# string -&gt; object conversion via dictionary
rr = resample(tsk("boston_housing"), lrn("regr.rpart"), cv3)
```

<i class="fas  fa-arrow-right "></i> Aggregated performance


```r
rr$aggregate(msr("regr.mse"))
```

```
regr.mse 
3.452013 
```

---
class: inverse, center, middle

# Benchmarking

---
# Benchmarking

<i class="fas  fa-info-circle "></i> `benchmark()` runs `resample()` on multiple learners and multiple tasks

<i class="fas  fa-arrow-right "></i> A sensible choice is usually the combination of all components in an exhaustive grid:


```r
design = benchmark_grid(
  tasks = list(tsk("iris"), tsk("spam")),
  learners = list(lrn("classif.featureless"), lrn("classif.rpart")),
  resamplings = rsmp("cv")
)
design
```

```
            task                     learner     resampling
1: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingCV&gt;
2: &lt;TaskClassif&gt;       &lt;LearnerClassifRpart&gt; &lt;ResamplingCV&gt;
3: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingCV&gt;
4: &lt;TaskClassif&gt;       &lt;LearnerClassifRpart&gt; &lt;ResamplingCV&gt;
```

---

# Benchmarking (cont.)

<i class="fas  fa-arrow-right "></i> Execute `benchmark()` on the defined design


```r
bmr = benchmark(design, store_models = TRUE)
bmr
```

```
&lt;BenchmarkResult&gt; of 40 rows with 4 resampling runs
 nr task_id          learner_id resampling_id iters warnings errors
  1    iris classif.featureless            cv    10        0      0
  2    iris       classif.rpart            cv    10        0      0
  3    spam classif.featureless            cv    10        0      0
  4    spam       classif.rpart            cv    10        0      0
```

<i class="fas  fa-arrow-right "></i> Calculate aggregated performance measure


```r
aggr = bmr$aggregate(msr("classif.acc"))
aggr[, 3:7]
```

```
   task_id          learner_id resampling_id iters classif.acc
1:    iris classif.featureless            cv    10   0.2066667
2:    iris       classif.rpart            cv    10   0.9333333
3:    spam classif.featureless            cv    10   0.6059511
4:    spam       classif.rpart            cv    10   0.8919744
```

---

# Benchmarking (cont.)

<i class="fas  fa-arrow-right "></i> Retrieve objects from the `BenchmarkResult`:


```r
# ResampleResult from 2nd configuration in the design (rpart on iris)
bmr$resample_result(2) -&gt; rr
# confusion matrix
rr$prediction()$confusion
```

```
            truth
response     setosa versicolor virginica
  setosa         50          0         0
  versicolor      0         45         5
  virginica       0          5        45
```

```r
# Average feature importance
library(magrittr)
sapply(rr$data$learner, function(x) x$importance()) %&gt;% apply(1, mean)
```

```
Petal.Length  Petal.Width Sepal.Length  Sepal.Width 
    80.38783     73.93299     49.17892     32.42676 
```

---
class: inverse, center, middle

# Tuning

---

# Tuning

* Algorithms: _Grid Search_, _Random Search_, _Simulated Annealing_
* In process: _Bayesian Optimization_, _iterated F-racing_, _EAs_
* Budget via class `Terminator`: iterations, performance, runtime, real time
* Nested resampling via class `AutoTuner`

.small[

```r
library(mlr3learners)
library(mlr3tuning)
ps = ParamSet$new(list(
  ParamInt$new("min.node.size", lower = 1, upper = 10),
  ParamInt$new("mtry", lower = 10, upper = 50)
))

at = AutoTuner$new(
  learner = lrn("classif.ranger"), # Random Forest
  resampling = rsmp("cv", folds = 3), # inner resampling
  measures = msr("classif.acc"),
  tune_ps = ps, terminator = term("evals", n_evals = 2), 
  tuner = tnr("random_search")
)
```


```r
resample(tsk("spam"), learner = at, rsmp("holdout")) # outer resampling
```

]

---

# AutoTuner in Benchmark

.pull-left[
<i class="fas  fa-arrow-right "></i> Compare tuned learner vs. default learner:


```r
design = benchmark_grid(
  tasks = tsk("spam"),
  learners = list(
    at,
    lrn("classif.ranger")
  ),
  resamplings = rsmp("cv", folds = 5))
bmr = benchmark(design)
```
]

.pull-right[
<i class="fas  fa-info-circle "></i> _mlr3viz_ package contains `autoplot()` functions for some `mlr3` objects.


```r
library(mlr3viz)
autoplot(bmr, 
  measure = msr("classif.acc"))
```

&lt;img src="whyr2019_mlr3_files/figure-html/whyr2019-mlr3-35-1.svg" width="90%" style="display: block; margin: auto;" /&gt;
]

---

class: inverse, center, middle

# mlr3pipelines

---
# Pipelines

So far *mlr3* just modeled this part:
&lt;div style="
    border: solid #ff0000 10px;
    width: 189px;
    height: 75px;
    position: fixed;
    left: 324px;
    top: 416px;"&gt;&lt;/div&gt;


Step 1: Use pipelines for preprocessing (feature extraction/selection, missing data imputation etc.).

Step 2: Use pipelines for ensemble models.

---

# Preprocessing Pipelines

.pull-left[
We can put preprocessing *PipeOps* in front of a Learner

![pipeline example](assets/mlr3pipelines-slide12.svg)

.small[


```r
library(mlr3pipelines)

mlr_pipeops
```

```
&lt;DictionaryPipeOp&gt; with 38 stored values
Keys: boxcox, branch, chunk, classbalancing,
  classifavg, colapply, copy, encode,
  encodelmer, featureunion, filter, histbin,
  ica, imputehist, imputemean, imputemedian,
  imputenewlvl, imputesample, kernelpca,
  learner, learner_cv, missind, modelmatrix,
  mutate, nop, pca, quantilebin, regravg,
  removeconstants, scale, scalemaxabs,
  scalerange, select, smote, spatialsign,
  subsample, unbranch, yeojohnson
```
]
]

.pull-right[
Use **`%&gt;&gt;%`** to connect *PipeOps*:
.small[

```r
scale = po("scale") # mlr_pipeops$get("scale")
encode = po("encode")
impute = po("imputemean")
learner = po("learner",
  lrn("classif.kknn", scale = FALSE))
grph = scale %&gt;&gt;% encode %&gt;&gt;% impute %&gt;&gt;% learner
(glrn = GraphLearner$new(grph))
```

```
&lt;GraphLearner:scale.encode.imputemean.classif.kknn&gt;
* Model: -
* Parameters: encode.method=one-hot,
  classif.kknn.scale=FALSE
* Packages: stats
* Predict Type: response
* Feature types: logical, integer, numeric,
  character, factor, ordered, POSIXct
* Properties: importance, missings, multiclass,
  oob_error, selected_features, twoclass,
  weights
```
]
]



---

# Access Hyperparameters of PipeOps

&lt;!-- Just like learners, *PipeOps* have hyperparameters that control their behavior. --&gt;

&lt;!-- Change hyperparameter values in ... --&gt;
&lt;!-- 1. construction of *PipeOp* --&gt;
&lt;!--   ```{r, results='hide'} --&gt;
&lt;!-- po("impute", method_num = "sample") --&gt;
&lt;!--   ``` --&gt;
&lt;!-- 2. *PipeOp* before building *Graph* --&gt;
&lt;!--   ```{r, results='hide'} --&gt;
&lt;!-- impute$param_set$values$method_num = "sample" --&gt;
&lt;!--   ``` --&gt;
&lt;!-- 3. *Graph* before building *GraphLearner* --&gt;
&lt;!--   ```{r, results='hide'} --&gt;
&lt;!-- grph$pipeops$impute$param_set$values$method_num = "sample" --&gt;
&lt;!--   ``` --&gt;
&lt;!-- 4. *GraphLearner*   --&gt;
&lt;!--   <i class="fas  fa-info-circle "></i> Note the prefix of the *PipeOp* name! --&gt;
&lt;!--   ```{r} --&gt;
&lt;!-- glrn$param_set$values$impute.method_num = "sample" # &lt;&lt; --&gt;
&lt;!--   ``` --&gt;

---

# Train Graph Learner

.pull-left[
<i class="fas  fa-arrow-right "></i> Train the *GraphLearner* like every other _mlr3_ learner:


```r
tsk = tsk("iris")
glrn$train(tsk, 
  row_ids = seq(1, 150, by = 2))
```

Most *PipeOps* have parameters (just like a trained model), which are fit to the training data:

![pipeline parameters](assets/mlr3pipelines-slide2.svg)
]

.pull-right[

<i class="fas  fa-info-circle "></i> The scaling factors `center` `\(\mu\)` and `scale` `\(\sigma\)` are fit on the *training* data, and applied to new data that passes through the *PipeOp* for *prediction*.

.small[

```r
glrn$model$scale[c("center", "scale")]
```

```
$center
Petal.Length  Petal.Width Sepal.Length  Sepal.Width 
    3.776000     1.218667     5.840000     3.064000 

$scale
Petal.Length  Petal.Width Sepal.Length  Sepal.Width 
   1.7829402    0.7907734    0.8058905    0.4354681 
```

```r
p = glrn$predict(tsk, 
  row_ids = seq(2, 150, by = 2))
```
]
]

---

# Train Graph Learner (cont.)

1. `glrn$train()` passes the training data through all _PipeOps_.  
   Each *PipeOp* transforms the data and saves the transformation parameter.
2. `glrn$predict()` passes the new data through all _PipeOps_  
   Each *PipeOp* applies the transformation with the stored transformation parameters.  
   This ensures, that new data is transformed exactly as the training data.
   
![trained graph learner](assets/mlr3pipelines-slide20.svg)

---

# PipeOp Parameters

<i class="fas  fa-arrow-right "></i> Resample the complete preprocessing pipeline:


```r
(rr = resample(tsk, glrn, rsmp("cv"), store_models = TRUE))
```

```
&lt;ResampleResult&gt; of 10 iterations
* Task: iris
* Learner: scale.encode.imputemean.classif.kknn
* Performance: 0.047 [classif.ce]
* Warnings: 0 in 0 iterations
* Errors: 0 in 0 iterations
```

Each model is scaled according to its training data:


```r
rr$learners[[1]]$model$scale$center
```

```
Petal.Length  Petal.Width Sepal.Length  Sepal.Width 
    3.896296     1.256296     5.902222     3.060741 
```

```r
rr$learners[[10]]$model$scale$center
```

```
Petal.Length  Petal.Width Sepal.Length  Sepal.Width 
    3.695556     1.170370     5.825185     3.053333 
```

<i class="fas  fa-info-circle "></i> Remember: Transforming the data before cross-validating skewed results. Transforming the data independently for training and test leads to wrong results!

---

# Tune Parameters of PipeOps

.small[

.pull-left[
We access the *PipeOp* hyperparameter through the parameter with its prefix in the *GraphLearner*.


```r
ps = ParamSet$new(list(
  ParamLgl$new("scale.scale"),
  ParamInt$new("classif.kknn.k",
    lower = 1, upper = 3)
))
```

<i class="fas  fa-info-circle "></i> *Grid search* terminates once the grid is completed, hence a termination criterion is optional.


```r
instance = TuningInstance$new(
  task = tsk("iris"),
  learner = glrn,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.acc"),
  param_set = ps,
  terminator = term("none")
)
```
]

.pull-right[

```r
tt = tnr("grid_search")
tt$tune(instance)
instance$archive()[, .(scale.scale,
  classif.kknn.k, classif.acc)]
```

```
   scale.scale classif.kknn.k classif.acc
1:       FALSE              2   0.9666667
2:        TRUE              2   0.9533333
3:        TRUE              1   0.9533333
4:       FALSE              3   0.9666667
5:        TRUE              3   0.9533333
6:       FALSE              1   0.9666667
```

```r
instance$result_config
```

```
$scale.scale
[1] FALSE

$classif.kknn.k
[1] 1
```
]
]

---

# Advaced Pipelines

Pipelines become exiting, once you can modify the path of the data using these *PipeOps*:

&lt;div style="display: block;width: 150px;height: 130px;position: absolute;background-color: #FAFAFACC; left: 200px"&gt;&lt;/div&gt;
![PipeOps](assets/mlr3pipelines-slide6.svg)

**Branching** and **Copy** have multiple outputs, so the next *PipeOps* needs multiple inputs.
To create these...

---

# Branching

1. Combine multiple *PipeOps* one below each other:  
  .img70[![mlr3pipelines union](assets/mlr3pipelines-slide4.svg)]
2. Replicate a *PipeOp* N times one below each other:  
  .img70[![mlr3pipelines replicate](assets/mlr3pipelines-slide5.svg)]  
  <i class="fas  fa-info-circle "></i> This is especially useful to build bagged learners in combination with *model averaging* (`regravg` or `classifavg`) afterwards.
  
---

# Control Branching through Parameters

.pull-left[

```r
library(mlr3filters)

pca = po("pca", rank. = 2)
filter = po("filter", flt("variance"),
  filter.nfeat = 2)

branch = po("branch",
  c("pca", "filter", "nop"))

union = gunion(
  list(pca, filter, po("nop")))

grph = branch %&gt;&gt;%
  union %&gt;&gt;%
  po("unbranch") %&gt;&gt;%
  po("learner", lrn("classif.kknn"))

plot(grph)
```
]

.pull-right[

]

---

# Tune Branching

.pull-left[

```r
glrn = GraphLearner$new(grph)

ps = ParamSet$new(list(
  ParamFct$new("branch.selection",
    levels = c("pca", "filter", "nop")),
  ParamInt$new("classif.kknn.k",
    lower = 1, upper = 3)
))

instance = TuningInstance$new(
  task = tsk("iris"),
  learner = glrn,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.acc"),
  param_set = ps,
  terminator = term("none")
)
```
]

.pull-right[

```r
tt = tnr("grid_search")
tt$tune(instance)
instance$result_config
```

```
$branch.selection
[1] "nop"

$classif.kknn.k
[1] 2
```
<i class="fas  fa-info-circle "></i> Tuning hyperparameters of *PipeOps* that *can* be active needs dependent parameters. 
Check the [mlr3book paradox chapter](https://mlr3book.mlr-org.com/paradox.html).
]

---
# Build Ensemble with PipeOps

.pull-left[
Instead of branching we can *copy*, to use multiple *PipeOps* on the same data:


```r
grph = po("copy", 2) %&gt;&gt;%
  gunion(list(
    po("learner_cv", lrn("regr.rpart")),
    po("nop")
  )) %&gt;&gt;%
  po("featureunion") %&gt;&gt;%
  po("learner", lrn("regr.lm"))
plot(grph)
```
]

.pull-right[

]

---

# Compare Pipe to normal Learner


```r
glrn = GraphLearner$new(grph)
design = benchmark_grid(
  tasks = tsk("mtcars"),
  learners = list(glrn, lrn("regr.lm")),
  resamplings = rsmp("cv"))
bmr = benchmark(design = design, store_models = TRUE)
bmr$aggregate()[, -(1:3)]
```

```
                                 learner_id resampling_id iters regr.mse
1: copy.regr.rpart.nop.featureunion.regr.lm            cv    10 21.23891
2:                                  regr.lm            cv    10 10.63873
```

---

# _PipeOps_ not covered by this tutorial

* Ensembles with `subsample`
* Feature generation with `mutate`
* Feature manipulation with `colapply`
* Feature selection with `select`
* Class balancing with `classbalancing`
* Prediction combining with `classifavg` and `regravg`
* and more.

<i class="fas  fa-info-circle "></i> Check the [mlr3book pipelines chapter](https://mlr3book.mlr-org.com/pipelines.html) for exhaustive explenations and more examples.

---

class: inverse, center, middle

# mlr3 Advanced Usage

---

# Internal Data Structure

All result objects (`resample()`, `benchmark()`, tuning, ...) share the same structure:


```r
as.data.table(rr)
```

```
             task        learner     resampling iteration prediction
 1: &lt;TaskClassif&gt; &lt;GraphLearner&gt; &lt;ResamplingCV&gt;         1     &lt;list&gt;
 2: &lt;TaskClassif&gt; &lt;GraphLearner&gt; &lt;ResamplingCV&gt;         2     &lt;list&gt;
 3: &lt;TaskClassif&gt; &lt;GraphLearner&gt; &lt;ResamplingCV&gt;         3     &lt;list&gt;
 4: &lt;TaskClassif&gt; &lt;GraphLearner&gt; &lt;ResamplingCV&gt;         4     &lt;list&gt;
 5: &lt;TaskClassif&gt; &lt;GraphLearner&gt; &lt;ResamplingCV&gt;         5     &lt;list&gt;
 6: &lt;TaskClassif&gt; &lt;GraphLearner&gt; &lt;ResamplingCV&gt;         6     &lt;list&gt;
 7: &lt;TaskClassif&gt; &lt;GraphLearner&gt; &lt;ResamplingCV&gt;         7     &lt;list&gt;
 8: &lt;TaskClassif&gt; &lt;GraphLearner&gt; &lt;ResamplingCV&gt;         8     &lt;list&gt;
 9: &lt;TaskClassif&gt; &lt;GraphLearner&gt; &lt;ResamplingCV&gt;         9     &lt;list&gt;
10: &lt;TaskClassif&gt; &lt;GraphLearner&gt; &lt;ResamplingCV&gt;        10     &lt;list&gt;
```

---

# Internal Data Structure

#### Combining R6 and data.table
* Not the objects are stored, but pointers to them

* Inexpensive to work on:
  * `rbind()`: copying R6 objects &amp;wedgeq; copying pointers
  * `cbind()`: `data.table()` over-allocates columns, no copies
  * `[i, ]`: lookup row (possibly hashed), create a list of pointers
  * `[, j]`: direct access to list element

---

# Control of Execution

<i class="fas  fa-arrow-right "></i> Parallelization

```r
future::plan("multicore")
benchmark(grid)
```
* runs each resampling iteration as a job&lt;br/&gt;
* also allows nested resampling (although not needed here)

<i class="fas  fa-arrow-right "></i>  Encapsulation


```r
ctrl = mlr_control(encapsulate_train = "callr")
benchmark(grid, ctrl = ctrl)
```
* Spawns a separate R process to train the learner
* Learner may segfault without tearing down the master session
* Logs are captured
* Possibility to have a fallback learner to create predictions

---

# Out-of-memory Data

* Task stores data in a `DataBackend`:
    * `DataBackendDataTable`: Default backend for dense data (in-memory)
    * `DataBackendMatrix`: Backend for sparse numerical data (in-memory)
    * `mlr3db::DataBackendDplyr`: Backend for many DBMS (out-of-memory)
    * `DataBackendCbind`: Combine backends thorugh `task$cbind(backend)` (virtual)
    * `DataBackendRbind`: Combine backends thorugh `task$rbind(backend)` (virtual)
    
* Backends are immutable
    * Filtering rows or selecting columns just modifies the "view" on the data
    * Multiple tasks can share the same backend
    
* Example: Interface a read-only MariaDB with `DataBackendDplyr`, add generated features via `DataBackendDataTable`

---

# Current state

https://github.com/mlr-org/mlr3/wiki/CI-Status

https://github.com/mlr-org/mlr3/wiki/Extension-Packages


Want to contribute?  
[mlr3.mlr-org.com](https://mlr3.mlr-org.com)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
