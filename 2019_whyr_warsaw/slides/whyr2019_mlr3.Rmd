---
title: "mlr3"
subtitle: "Modern machine learning in R"
author: "Michel Lang, Bernd Bischl, Jakob Richter, Patrick Schratz, Martin Binder"
output:
  xaringan::moon_reader:
    css: ["metropolis", "robot-fonts", "extra.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---
```{r, include = FALSE}
CHAR_WIDTH = 100
FIG_WIDTH = 9
FIG_HEIGHT = 4
library(paradox)
library(mlr3)
requireNamespace("fansi")
requireNamespace("icon") # remotes::install_github("ropenscilabs/icon")
options(
    crayon.enabled = TRUE,
    datatable.print.class = FALSE,
    datatable.print.keys = FALSE,
    width = CHAR_WIDTH
)
knitr::opts_chunk$set(
  comment = NA,
  width = CHAR_WIDTH,
  fig.height = FIG_HEIGHT,
  fig.width = FIG_WIDTH,
  fig.align = "center",
  dev = 'svg',
  out.width = "90%",
  cache = FALSE #because it does not work wiht mlr3/R6
)
lgr::get_logger("mlr3")$set_threshold("warn")
```

# What is machine learning?

Supervised 
* **Classification**
  * Binary
  * Multiclass
* **Regression**
* Survival (Time To Event)

Unsupervised
* Clustering

---

# What is a machine learning pipeline?

```{r fig_pipelines, echo = FALSE}
knitr::include_graphics("assets/ml-workflow.png", dpi = 50)
```

`r icon::fa("question-circle")` Why are pipelines necessary?  
`r icon::fa("exclamation-circle")` Gives reproducible process that can be validated (through nested crossvalidation).


---

# Machine Learning in R

The **good** news:
- CRAN serves hundreds of packages for machine learning
- Many packages for single specific ML-methods
  - `randomForest`, `ranger`
  - `e1070::svm`
  - `kknn`
  - `xgboost`
  - ...
- Often compliant to the unwritten interface definition:
```{r, eval=FALSE}
model = fit(target ~ ., data = train.data, ...)
predictions = predict(model, newdata = test.data, ...)
```

---

# Machine Learning in R

The **bad** news:
- Some packages' API is "just different"
- Functionality is always package or model-dependent, even though the procedure might be general
- Capabilities often not clearly documented
  - "Can this method handle missing/categorical values?"
- ML workflow not standardized in R
  - Cross Validation
  - Hyperparameter Tuning
  - Self implementations are error prone

---

# Motivation: (old) mlr

- Unified interface for the basic building blocks: **tasks**, **learners**,
  **hyperparameters**.

- Simplifying repetitive tasks:
  - cross-validation
  - hyperparameter tuning
  - comparision of different methods
  - parallelization  

- Project home page: https://github.com/mlr-org/mlr
- 8-10 main developers
  - + even more contributors
  - + 5 GSOC projects since 2015
- About 30K lines of code, 8K lines of unit tests

---

# What (old) mlr became

Meta framework for everything machine learning (visualization, tuning, feature selection, preprocessing, bagging, ensembles, ...)

* Monolithic package
* Interfaces > 150 learners  
  `r icon::fa("arrow-right")` Dependencies (direct / recursive): 119 / 1436  
  `r icon::fa("arrow-right")` Unit tests take > 2h  
  `r icon::fa("arrow-right")` Package developers changed their API and (unknowingly) broke mlr  
* High barrier for new contributors
* S3 reaches its limitations in larger software projects
* Many specialized hard to find functions `getBMRAggrPerformances()`

#### Further Design Issues

* Only works on in-memory data
* No nested parallelization
---
# mlr3

* Overcome limitations of S3 with the help of **R6**
  * Truly object-oriented (OO): data and methods together
  * Inheritance
  * Reference semantics
  
* Embrace **data.table**, both for arguments and for internal data structures
  * Fast operations for tabular data
  * Better support for list columns to arrange complex objects in a tabular structure
  * Reference semantics
  
* Be **light on dependencies**. Direct and recursive dependencies:
  * `R6`, `data.table`, `Metrics`, `lgr`
  * Some self-maintained packages (`backports`, `checkmate`, ...)

---
# consequences
* steeper learning curve to learn mlr3
* multiple packages for different functionality
  * mlr3learners
  * mlr3pipelines
  * mlr3tuning
  * mlr3filters
  * mlr3viz
  * ...
* easier extendable
  * inherit classes and overwrite functionality



---
class: inverse, center, middle

# mlr3

---

# Building Blocks
```{r fig_building_blocks, echo = FALSE}
knitr::include_graphics("assets/ml_abstraction_colors.svg.png", dpi = 50)
```

---

# Tasks


`r icon::fa("arrow-right")` Create your own task
```{r}
TaskClassif$new("iris", iris, target = "Species")
```
    
`r icon::fa("arrow-right")` Retrieve a predefined task from the task dictionary

```{r}
mlr_tasks
task = mlr_tasks$get("iris")
```

---
# Learner

`r icon::fa("arrow-right")` Retrieve a predefined learner from the learner dictionary

```{r}
mlr_learners
```
`r icon::fa("info-circle")` More learners are available after loading `mlr3learners`.

.small[
```{r}
learner = mlr_learners$get("classif.rpart")
learner
```
]



---
# Learner

`r icon::fa("arrow-right")` Querying and setting hyperparameters

```{r}
# query
learner$param_set
# set
learner$param_set$values = list(xval = 0, cp = 0.1)
```
    
---

# Learner

`r icon::fa("arrow-right")` Training
```{r}
task = mlr_tasks$get("iris")
learner$train(task, row_ids = seq(1, to = 150, by = 2))
```
`r icon::fa("info-circle")` This changes the learner in-place, model is now stored inside the learner.

---
# Learner

`r icon::fa("arrow-right")` Accessing the learner model
.small[
```{r}
learner$model
```
]

`r icon::fa("smile")` Unified interface, to acess *feature importance*, *selected features* etc. for all learners. 
.small[
```{r}
learner$importance()
learner$selected_features()
```
]

---
# Prediction

`r icon::fa("arrow-right")` Generate predictions
```{r}
p = learner$predict(task, row_ids = seq(2, to = 150, by = 2))
head(as.data.table(p), 3)
```


`r icon::fa("arrow-right")` Confusion matrix
```{r}
p$confusion
```


---

# Measure
`r icon::fa("arrow-right")` Retrieve a predefined measure from the measure dictionary
.small[
```{r,include=FALSE}
options(width = CHAR_WIDTH*1.2)
```
```{r}
mlr_measures
(measure = mlr_measures$get("classif.acc"))
```
```{r, include=FALSE}
options(width = CHAR_WIDTH)
```

]

`r icon::fa("arrow-right")` Calculate performance
```{r}
p$score(measure)
```

---
# Resample


`r icon::fa("arrow-right")` Automate train/test splitting and predict with *resampling*.
.small[
```{r}
mlr_resamplings
resampling = mlr_resamplings$get("cv")
resampling$param_set$values$folds = 3
rr = resample(task, learner, resampling)
rr$performance() 
rr$aggregate()
```
]

`r icon::fa("info-circle")` The default measure for classif tasks is the *classification error*.

---
# Recap: Builing Blocks
```{r fig_building_blocks, echo = FALSE}
```

---

# Dictionaries

Dictionaries store often used objects.

|Dictionary|Helper|  |
|:---------|:-----|--|
|`mlr_tasks`|`tsk()`|Example tasks (iris, spam, ...)|
|`mlr_task_generators`|`tgen()`|Syhnthetic task generators|
|`mlr_learners`|`lrn()`|learners|
|`mlr_measures`|`msr()`|measures (acc, auc, mse, ...)|
|`mlr_resamplings`|`rsmp()`|resampling strategies (cv, holdout, bootstrap, ...)|


`r icon::fa("info-circle")` Dictionaries can get populated by add-on packages (eg. `mlr3learners`).

```{r}
mlr_tasks
task = mlr_tasks$get("spam")
task = tsk("spam") #helper
```

---
# Recap: Example Train/Test

```{r, include = FALSE}
library(mlr3)
set.seed(1)
```


```{r}
# Create learning task
task_iris = TaskClassif$new(id = "iris", backend = iris, target = "Species")

# Load learner from dictionary
learner = lrn("classif.rpart")

# Set learner hyperparameter
learner$param_set$values$cp = 0.01

# Create train/test split
train_set = sample(task_iris$nrow, 0.8 * task_iris$nrow)
test_set = setdiff(seq_len(task_iris$nrow), train_set)

# Train the model
learner$train(task_iris, row_ids = train_set)

# Predict data
prediction = learner$predict(task_iris, row_ids = test_set)
```

---
# Example Train/Test (cont.)

.small[
```{r}
prediction
# calculate performance
prediction$confusion
prediction$score(msr("classif.acc"))
```
]

---
# Recap: Example Resample


`r icon::fa("arrow-right")` Resampling Object

```{r}
cv3 = rsmp("cv", folds = 3)
```

Splits into train/test are efficiently stored and can be accessed with `$train_set(i)` and `$test_set(i)`.

`r icon::fa("arrow-right")` Resample a regression tree on the Boston housing data using a 3-fold CV

```{r}
# string -> object conversion via dictionary
rr = resample(tsk("boston_housing"), lrn("regr.rpart"), cv3)
```

`r icon::fa("arrow-right")` Aggregated performance

```{r}
rr$aggregate(msr("regr.mse"))
```


---
class: inverse, center, middle

# Benchmarking

---

# Benchmarking

`r icon::fa("info-circle")` `benchmark()` runs resample on multiple learners and multiple tasks

`r icon::fa("arrow-right")` A sensible choice is usually the combination of all components in an exhaustive gird:
```{r benchmark1_design}
design = benchmark_grid(
  tasks = list(tsk("iris"), tsk("spam")),
  learners = list(lrn("classif.featureless"), lrn("classif.rpart")),
  resamplings = rsmp("cv")
)
design
```
---

# Benchmarking (cont.)

`r icon::fa("arrow-right")` Execute `benchmark()` on the defined design
.small[
```{r benchmark1}
bmr = benchmark(design, store_models = TRUE)
bmr
```

`r icon::fa("arrow-right")` Calculate aggregated performance measure
```{r}
aggr = bmr$aggregate(msr("classif.acc"))
aggr[, 3:7]
```
]

---

# Benchmarking (cont.)


`r icon::fa("arrow-right")` Retrieve objects from the `BenchmarkResult`:

```{r}
# ResampleResult from 2nd configuration in the design (rpart on iris)
bmr$resample_result(2) -> rr
# confusion matrix
rr$prediction()$confusion
# Average feature importance
library(magrittr)
sapply(rr$data$learner, function(x) x$importance()) %>% apply(1, mean)
```

---
class: inverse, center, middle

# Tuning

---

# Tuning

* Algorithms: _Grid Search_, _Random Search_, _Simulated Annealing_
* In process: _Bayesian Optimization_, _iterated F-racing_, _EAs_
* Budget via class `Terminator`: iterations, performance, runtime, real time
* Nested resampling via class `AutoTuner`

.small[
```{r at_resample}
library(mlr3learners); library(mlr3tuning)
ps = ParamSet$new(list(
  ParamInt$new("min.node.size", lower = 1, upper = 10),
  ParamInt$new("mtry", lower = 10, upper = 50)
))

at = AutoTuner$new(
  learner = lrn("classif.ranger"), # Random Forest
  resampling = rsmp("cv", folds = 3), # inner resampling
  measures = msr("classif.acc"),
  tune_ps = ps, terminator = term("evals", 2), tuner = tnr("random_search")
)
```
```{r, eval = FALSE}
resample(tsk("spam"), learner = at, rsmp("holdout")) # outer resampling
```
]

---

# AutoTuner in Benchmark

.pull-left[
`r icon::fa("arrow-right")` Compare tuned learner vs. default learner:

```{r at_benchmark, eval = FALSE}
design = benchmark_grid(
  tasks = tsk("spam"), 
  learners = list(
    at, 
    lrn("classif.ranger")
  ), 
  resamplings = rsmp("cv", folds = 5))
bmr = benchmark(design)
```
]

.pull-right[
`r icon::fa("info-circle")` `mlr3viz` package contains `autoplot()` functions for some `mlr3` objects.
.small[
```{r fig_at_benchmark, fig.width=0.5*FIG_WIDTH}
library(mlr3viz)
autoplot(bmr, measure = msr("classif.acc"))
```
]
]


---

class: inverse, center, middle

# mlr3pipelines

---
# Pipelines

So far *mlr3* just modeled this part:
<div style="
    border: solid #ff0000 10px;
    width: 189px;
    height: 75px;
    position: fixed;
    left: 324px;
    top: 416px;"></div>
```{r fig_pipelines, echo = FALSE}
```

Step 1: Use pipelines for preprocessing (feature extraction/selection, missing data imputation etc.).
Step 2: Use pipelines for ensemble models (later).


---

class: inverse, center, middle

# Advanced Usage

---

# Internal Data Structure

All result objects (`resample()`, `benchmark()`, tuning, ...) share the same structure:

```{r}
as.data.table(rr)
```


#### Combining R6 and data.table
* Not the objects are stored, but pointers to them

* Inexpensive to work on:
  * `rbind()`: copying R6 objects &wedgeq; copying pointers
  * `cbind()`: `data.table()` over-allocates columns, no copies
  * `[i, ]`: lookup row (possibly hashed), create a list of pointers
  * `[, j]`: direct access to list element


---

# Control of Execution


`r icon::fa("arrow-right")` Parallelization
```{r, eval = FALSE}
future::plan("multicore")
benchmark(grid)
```
* runs each resampling iteration as a job<br/>
* also allows nested resampling (although not needed here)


`r icon::fa("arrow-right")`  Encapsulation

```{r, eval = FALSE}
ctrl = mlr_control(encapsulate_train = "callr")
benchmark(grid, ctrl = ctrl)
```
* Spawns a separate R process to train the learner
* Learner may segfault without tearing down the master session
* Logs are captured
* Possibility to have a fallback learner to create predictions


---

# Out-of-memory Data

* Task stores data in a `DataBackend`:
    * `DataBackendDataTable`: Default backend for dense data (in-memory)
    * `DataBackendMatrix`: Backend for sparse numerical data (in-memory)
    * `mlr3db::DataBackendDplyr`: Backend for many DBMS (out-of-memory)
    * `DataBackendCbind`: Combine backends thorugh `task$cbind(backend)` (virtual)
    * `DataBackendRbind`: Combine backends thorugh `task$rbind(backend)` (virtual)
    
* Backends are immutable
    * Filtering rows or selecting columns just modifies the "view" on the data
    * Multiple tasks can share the same backend
    
* Example: Interface a read-only MariaDB with `DataBackendDplyr`, add generated features via `DataBackendDataTable`

---

# Current state

https://github.com/mlr-org/mlr3/wiki/CI-Status
https://github.com/mlr-org/mlr3/wiki/Extension-Packages


Want to contribute?  
[mlr3.mlr-org.com](https://mlr3.mlr-org.com)

